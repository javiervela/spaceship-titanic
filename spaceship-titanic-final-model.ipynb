{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spaceship Titanic - Notebook\n",
    "\n",
    "<!-- TODO -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries. We will use:\n",
    "\n",
    "- `pandas` to load the data and manipulate it.\n",
    "- `scikit-learn` to build the model.\n",
    "<!-- TODO - `matplotlib` and `seaborn` to plot the data. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    RFE,\n",
    ")\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from featuretools import EntitySet, dfs\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler\n",
    "\n",
    "import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "DATA_DIR = f\"{CURRENT_DIR}/data\"\n",
    "TRAIN_DATA_FILE = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_DATA_FILE = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "TARGET_COLUMN = \"Transported\"\n",
    "ID_COLUMN = \"PassengerId\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SIZE = 0.2\n",
    "\n",
    "MISSING_VALUE = \"Missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data files into pandas dataframes\n",
    "train_data = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_data = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of data:\")\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data columns and types:\")\n",
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_COLUMNS = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "CATEGORICAL_COLUMNS = train_data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "\n",
    "leftover_columns = [\n",
    "    col\n",
    "    for col in train_data.columns\n",
    "    if col not in NUMERICAL_COLUMNS\n",
    "    and col not in CATEGORICAL_COLUMNS\n",
    "    and col != TARGET_COLUMN\n",
    "]\n",
    "assert not leftover_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numerical columns: {NUMERICAL_COLUMNS}\")\n",
    "print(f\"Categorical columns: {CATEGORICAL_COLUMNS}\")\n",
    "print(f\"Target column: {TARGET_COLUMN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = CATEGORICAL_COLUMNS.copy()\n",
    "categorical_columns.remove(ID_COLUMN)\n",
    "categorical_columns.remove(\"Cabin\")\n",
    "categorical_columns.remove(\"Name\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    train_data.groupby([col, TARGET_COLUMN]).size().unstack().plot(\n",
    "        kind=\"bar\", stacked=True\n",
    "    )\n",
    "    plt.title(f\"Count plot for {col} by {TARGET_COLUMN}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary statistics:\")\n",
    "display(train_data.describe())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# print(\"\\nCorrelation matrix:\")\n",
    "# sns.heatmap(train_data[NUMERICAL_COLUMNS].corr(), annot=True)\n",
    "# plt.show()\n",
    "\n",
    "print(\"\\nValue counts for categorical variables:\")\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(train_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset\n",
    "\n",
    "We need to clean the train and test datasets the same way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: pd.DataFrame):\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    # Convert columns to integer (with missing values)\n",
    "    for col in [\n",
    "        \"CabinNumber\",\n",
    "        \"CryoSleep\",\n",
    "        \"VIP\",\n",
    "        \"Transported\",\n",
    "        \"PassengerGroupId\",\n",
    "        \"PassengerIntraGroupId\",\n",
    "    ]:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Make PassengerId the index\n",
    "    data.set_index(ID_COLUMN, inplace=True)\n",
    "\n",
    "    # Drop columns\n",
    "    for col in [\n",
    "        \"Name\",\n",
    "        \"Cabin\",\n",
    "        \"PassengerGroupId\",\n",
    "        \"PassengerIntraGroupId\",\n",
    "    ]:\n",
    "        if col in data.columns:\n",
    "            data.drop(columns=col, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = clean_data(train_data)\n",
    "# test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATED_FEATURES = [\n",
    "    \"AmountSpentTotal\",\n",
    "    \"CabinDeck\",\n",
    "    \"CabinNumber\",\n",
    "    \"CabinSide\",\n",
    "    \"CabinMates\",\n",
    "    \"PassengerGroupSize\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_features(\n",
    "    data: pd.DataFrame,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create new features:\n",
    "    # - AmountSpentTotal: Total money spent in the ship's service\n",
    "    # - CabinDeck: Deck of the cabin\n",
    "    # - CabinNumber: Number of the cabin\n",
    "    # - CabinSide: Side of the cabin\n",
    "    # - CabinMates: Number of people in the same cabin\n",
    "    # - PassengerGroupSize: Group Size\n",
    "\n",
    "    # Get from kwargs the features to return\n",
    "    selected_features = [\n",
    "        feature for feature in CREATED_FEATURES if kwargs.get(f\"use_{feature}\", False)\n",
    "    ]\n",
    "\n",
    "    new_data = data.copy()\n",
    "    data = data.copy()\n",
    "\n",
    "    # Create new feature: Total money spent in the ship's service\n",
    "    new_data[\"AmountSpentTotal\"] = data[\n",
    "        [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "    ].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Create new feature: Mean money spent in the ship's service\n",
    "    # TODO is the same as the other one\n",
    "    # new_data[\"AmountSpentMean\"] = data[\n",
    "    #     [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "    # ].mean(axis=1, skipna=True)\n",
    "\n",
    "    # Create new features: Convert Cabin to three different columns (Deck, Number, Side)\n",
    "    new_data[[\"CabinDeck\", \"CabinNumber\", \"CabinSide\"]] = data[\"Cabin\"].str.split(\n",
    "        \"/\", expand=True\n",
    "    )\n",
    "\n",
    "    # Create new feature: Number of people in the same cabin\n",
    "    new_data[\"CabinMates\"] = data.groupby(\"Cabin\")[\"Cabin\"].transform(\"count\")\n",
    "\n",
    "    # Create new features: Group Id, Group Size, Intra Group Id,\n",
    "    new_data[[\"PassengerGroupId\", \"PassengerIntraGroupId\"]] = data[ID_COLUMN].str.split(\n",
    "        \"_\", expand=True\n",
    "    )\n",
    "    new_data[\"PassengerGroupSize\"] = new_data.groupby(\"PassengerGroupId\")[\n",
    "        \"PassengerGroupId\"\n",
    "    ].transform(\"count\")\n",
    "\n",
    "    # Only return the old features and the selected new ones\n",
    "    return pd.concat([data, new_data[selected_features]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = create_features(train_data)\n",
    "# test_data = create_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed_df = pipeline.fit_transform(train_data)\n",
    "print(train_data_transformed_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "- Input Data\n",
    "- Mark as \"Missing\"\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "- Make Categorical Columns Numerical\n",
    "  - One-Hot encoding\n",
    "  - Ordinal encoding\n",
    "- Scale Numerical Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CARDINALITY = 4\n",
    "\n",
    "\n",
    "def select_high_cardinality_categorical_features(df: pd.DataFrame):\n",
    "    hi_c_cat = df.select_dtypes(include=[\"object\"]).nunique() > MAX_CARDINALITY\n",
    "    features = hi_c_cat[hi_c_cat].index.tolist()\n",
    "    return features\n",
    "\n",
    "\n",
    "def select_low_cardinality_categorical_features(df: pd.DataFrame):\n",
    "    lo_c_cat = df.select_dtypes(include=[\"object\"]).nunique() <= MAX_CARDINALITY\n",
    "    features = lo_c_cat[lo_c_cat].index.tolist()\n",
    "    return features\n",
    "\n",
    "\n",
    "def select_numerical_features(df: pd.DataFrame):\n",
    "    return df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "\n",
    "# Combine handling missing values and preprocessing into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat_low_cardinality\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"impute\",\n",
    "                        SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "                    ),\n",
    "                    (\"to_num\", OneHotEncoder()),\n",
    "                ]\n",
    "            ),\n",
    "            select_low_cardinality_categorical_features,\n",
    "        ),\n",
    "        (\n",
    "            \"cat_high_cardinality\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"impute\",\n",
    "                        SimpleImputer(strategy=\"most_frequent\"),\n",
    "                    ),\n",
    "                    (\"to_num\", OneHotEncoder()),\n",
    "                ]\n",
    "            ),\n",
    "            select_high_cardinality_categorical_features,\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\"impute\", KNNImputer(n_neighbors=5)),\n",
    "                    (\"scale\", StandardScaler()),\n",
    "                ]\n",
    "            ),\n",
    "            select_numerical_features,\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# preprocessor.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data: pd.DataFrame, pipeline: Pipeline) -> pd.DataFrame:\n",
    "    X = data.drop(columns=[TARGET_COLUMN])\n",
    "    y = data[TARGET_COLUMN]\n",
    "\n",
    "    # Fit and transform the data using the pipeline\n",
    "    data_transformed = pipeline.fit_transform(X=X, y=y)\n",
    "\n",
    "    # Extract feature names from the preprocessor step\n",
    "    if \"preprocessor\" in pipeline.named_steps:\n",
    "        feature_names = pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = data.columns\n",
    "\n",
    "    # Extract the selected feature indices from the feature engineering step\n",
    "    if \"feature_engineering\" in pipeline.named_steps:\n",
    "        feature_selector = pipeline.named_steps[\"feature_engineering\"].named_steps[\n",
    "            \"feature_selection\"\n",
    "        ]\n",
    "        if isinstance(feature_selector, SelectFromModel):\n",
    "            support_mask = feature_selector.get_support()\n",
    "        elif isinstance(feature_selector, RFE):\n",
    "            support_mask = feature_selector.support_\n",
    "        else:\n",
    "            support_mask = np.ones(len(feature_names), dtype=bool)\n",
    "        selected_feature_names = [\n",
    "            name for name, selected in zip(feature_names, support_mask) if selected\n",
    "        ]\n",
    "    else:\n",
    "        selected_feature_names = feature_names\n",
    "\n",
    "    # Remove prefixes from the column names\n",
    "    selected_feature_names = [name.split(\"__\")[-1] for name in selected_feature_names]\n",
    "\n",
    "    # Convert the transformed data back to a DataFrame\n",
    "    data_transformed_df = pd.DataFrame(data_transformed, columns=selected_feature_names)\n",
    "\n",
    "    data_transformed_df[TARGET_COLUMN] = y.values\n",
    "\n",
    "    return data_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to transform the train_data\n",
    "train_data_transformed_df = transform_data(train_data, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Preprocessing Works\n",
    "\n",
    "- Check if no missing values after preprocessing\n",
    "- Check if all columns are numerical after preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "print(\"Number of missing values in transformed data:\")\n",
    "display(pd.DataFrame(train_data_transformed_df.isna().sum()).T)\n",
    "\n",
    "assert train_data_transformed_df.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all columns are numerical\n",
    "\n",
    "all_columns_numerical = train_data_transformed_df.select_dtypes(\n",
    "    include=[np.number]\n",
    ").columns.tolist()\n",
    "all_columns = train_data_transformed_df.columns.tolist()\n",
    "\n",
    "columns_not_numerical = (\n",
    "    set(all_columns) - set(all_columns_numerical) - set([TARGET_COLUMN])\n",
    ")\n",
    "print(f\"Columns not numerical: {columns_not_numerical}\")\n",
    "\n",
    "# Output the types of the non-numerical columns\n",
    "for col in columns_not_numerical:\n",
    "    print(\n",
    "        f\"Column: {col}, Type: {train_data_transformed_df[col].dtype}, First Value: {train_data_transformed_df[col].iloc[0]}\"\n",
    "    )\n",
    "\n",
    "assert columns_not_numerical == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASSO_CV = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"feature_selection\",\n",
    "            SelectFromModel(\n",
    "                LassoCV(cv=LASSO_CV, random_state=RANDOM_SEED, max_iter=10000)\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the feature engineering pipeline to the main pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_engineering\", feature_engineering),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to transform the train_data\n",
    "train_data_transformed_df = transform_data(train_data, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Correlation on Transformed Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_data_transformed_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=train_data_transformed_df.columns.tolist(),\n",
    "    yticklabels=train_data_transformed_df.columns.tolist(),\n",
    ")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the correlation matrix to only include the Target Column\n",
    "target_corr_matrix = corr_matrix[[TARGET_COLUMN]].sort_values(\n",
    "    by=TARGET_COLUMN, ascending=False\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(target_corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(f\"Correlation with {TARGET_COLUMN}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Parameter Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the train data into training and validation sets\n",
    "# X = train_data.drop(columns=[TARGET_COLUMN])\n",
    "# y = train_data[TARGET_COLUMN]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X, y, test_size=VALIDATION_SIZE, random_state=RANDOM_SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data into training and validation sets\n",
    "X_train = train_data.drop(columns=[TARGET_COLUMN])\n",
    "y_train = train_data[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_engineering\", feature_engineering),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=RANDOM_SEED,\n",
    "                max_depth=4,\n",
    "                criterion=\"friedman_mse\",\n",
    "                learning_rate=0.01,\n",
    "                min_samples_leaf=45,\n",
    "                min_samples_split=10,\n",
    "                n_estimators=1000,\n",
    "                subsample=0.9,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Evaluation with Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate all estimators in grid search with validation set\n",
    "# for estimator, _ in all_estimators_with_scores:\n",
    "#     pipeline.set_params(**estimator)\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     score = evaluate_model(pipeline, estimator, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Training and Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_pipeline = map_and_set_params(pipeline, best_params)\n",
    "best_pipeline = pipeline\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "X_test = test_data\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "test_data[TARGET_COLUMN] = y_pred.astype(bool)\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "# best_model = max(best_models.items(), key=lambda x: cross_val_score(x[1], X_train, y_train, cv=5).mean())[1]\n",
    "# test_predictions = best_model.predict(test_data)\n",
    "# test_data[TARGET_COLUMN] = test_predictions.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with only the ID_COLUMN and Predictions\n",
    "predictions_df = test_data.reset_index()[[ID_COLUMN, TARGET_COLUMN]]\n",
    "\n",
    "# Print predictions\n",
    "print(predictions_df)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv(f\"{DATA_DIR}/predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceship-titanic-UUL0AmCy-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
