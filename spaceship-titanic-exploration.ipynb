{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spaceship Titanic - Notebook\n",
    "\n",
    "<!-- TODO -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries. We will use:\n",
    "\n",
    "- `pandas` to load the data and manipulate it.\n",
    "- `scikit-learn` to build the model.\n",
    "<!-- TODO - `matplotlib` and `seaborn` to plot the data. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    f_classif,\n",
    "    mutual_info_classif,\n",
    "    RFE,\n",
    ")\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LassoCV, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from featuretools import EntitySet, dfs\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler\n",
    "\n",
    "import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "DATA_DIR = f\"{CURRENT_DIR}/data\"\n",
    "TRAIN_DATA_FILE = f\"{DATA_DIR}/train.csv\"\n",
    "TEST_DATA_FILE = f\"{DATA_DIR}/test.csv\"\n",
    "\n",
    "TARGET_COLUMN = \"Transported\"\n",
    "ID_COLUMN = \"PassengerId\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "VALIDATION_SIZE = 0.2\n",
    "\n",
    "MISSING_VALUE = \"Missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data files into pandas dataframes\n",
    "train_data = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_data = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of data:\")\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data columns and types:\")\n",
    "print(train_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_COLUMNS = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "CATEGORICAL_COLUMNS = train_data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "\n",
    "leftover_columns = [\n",
    "    col\n",
    "    for col in train_data.columns\n",
    "    if col not in NUMERICAL_COLUMNS\n",
    "    and col not in CATEGORICAL_COLUMNS\n",
    "    and col != TARGET_COLUMN\n",
    "]\n",
    "assert not leftover_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numerical columns: {NUMERICAL_COLUMNS}\")\n",
    "print(f\"Categorical columns: {CATEGORICAL_COLUMNS}\")\n",
    "print(f\"Target column: {TARGET_COLUMN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = CATEGORICAL_COLUMNS.copy()\n",
    "categorical_columns.remove(ID_COLUMN)\n",
    "categorical_columns.remove(\"Cabin\")\n",
    "categorical_columns.remove(\"Name\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    train_data.groupby([col, TARGET_COLUMN]).size().unstack().plot(\n",
    "        kind=\"bar\", stacked=True\n",
    "    )\n",
    "    plt.title(f\"Count plot for {col} by {TARGET_COLUMN}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary statistics:\")\n",
    "display(train_data.describe())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# print(\"\\nCorrelation matrix:\")\n",
    "# sns.heatmap(train_data[NUMERICAL_COLUMNS].corr(), annot=True)\n",
    "# plt.show()\n",
    "\n",
    "print(\"\\nValue counts for categorical variables:\")\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(train_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset\n",
    "\n",
    "We need to clean the train and test datasets the same way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data: pd.DataFrame):\n",
    "\n",
    "    data = data.copy()\n",
    "\n",
    "    # Convert columns to integer (with missing values)\n",
    "    for col in [\n",
    "        \"CabinNumber\",\n",
    "        \"CryoSleep\",\n",
    "        \"VIP\",\n",
    "        \"Transported\",\n",
    "        \"PassengerGroupId\",\n",
    "        \"PassengerIntraGroupId\",\n",
    "    ]:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_numeric(data[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Make PassengerId the index\n",
    "    data.set_index(ID_COLUMN, inplace=True)\n",
    "\n",
    "    # Drop columns\n",
    "    for col in [\n",
    "        \"Name\",\n",
    "        \"Cabin\",\n",
    "        \"PassengerGroupId\",\n",
    "        \"PassengerIntraGroupId\",\n",
    "    ]:\n",
    "        if col in data.columns:\n",
    "            data.drop(columns=col, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = clean_data(train_data)\n",
    "# test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATED_FEATURES = [\n",
    "    \"AmountSpentTotal\",\n",
    "    \"CabinDeck\",\n",
    "    \"CabinNumber\",\n",
    "    \"CabinSide\",\n",
    "    \"CabinMates\",\n",
    "    \"PassengerGroupSize\",\n",
    "]\n",
    "\n",
    "\n",
    "def create_features(\n",
    "    data: pd.DataFrame,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    # Create new features:\n",
    "    # - AmountSpentTotal: Total money spent in the ship's service\n",
    "    # - CabinDeck: Deck of the cabin\n",
    "    # - CabinNumber: Number of the cabin\n",
    "    # - CabinSide: Side of the cabin\n",
    "    # - CabinMates: Number of people in the same cabin\n",
    "    # - PassengerGroupSize: Group Size\n",
    "\n",
    "    # Get from kwargs the features to return\n",
    "    selected_features = [\n",
    "        feature for feature in CREATED_FEATURES if kwargs.get(f\"use_{feature}\", False)\n",
    "    ]\n",
    "\n",
    "    new_data = data.copy()\n",
    "    data = data.copy()\n",
    "\n",
    "    # Create new feature: Total money spent in the ship's service\n",
    "    new_data[\"AmountSpentTotal\"] = data[\n",
    "        [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "    ].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Create new feature: Mean money spent in the ship's service\n",
    "    # TODO is the same as the other one\n",
    "    # new_data[\"AmountSpentMean\"] = data[\n",
    "    #     [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "    # ].mean(axis=1, skipna=True)\n",
    "\n",
    "    # Create new features: Convert Cabin to three different columns (Deck, Number, Side)\n",
    "    new_data[[\"CabinDeck\", \"CabinNumber\", \"CabinSide\"]] = data[\"Cabin\"].str.split(\n",
    "        \"/\", expand=True\n",
    "    )\n",
    "\n",
    "    # Create new feature: Number of people in the same cabin\n",
    "    new_data[\"CabinMates\"] = data.groupby(\"Cabin\")[\"Cabin\"].transform(\"count\")\n",
    "\n",
    "    # Create new features: Group Id, Group Size, Intra Group Id,\n",
    "    new_data[[\"PassengerGroupId\", \"PassengerIntraGroupId\"]] = data[ID_COLUMN].str.split(\n",
    "        \"_\", expand=True\n",
    "    )\n",
    "    new_data[\"PassengerGroupSize\"] = new_data.groupby(\"PassengerGroupId\")[\n",
    "        \"PassengerGroupId\"\n",
    "    ].transform(\"count\")\n",
    "\n",
    "    # Only return the old features and the selected new ones\n",
    "    return pd.concat([data, new_data[selected_features]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = create_features(train_data)\n",
    "# test_data = create_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed_df = pipeline.fit_transform(train_data)\n",
    "print(train_data_transformed_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "- Input Data\n",
    "- Mark as \"Missing\"\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "- Make Categorical Columns Numerical\n",
    "  - One-Hot encoding\n",
    "  - Ordinal encoding\n",
    "- Scale Numerical Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CARDINALITY = 4\n",
    "\n",
    "\n",
    "def select_high_cardinality_categorical_features(df: pd.DataFrame):\n",
    "    hi_c_cat = df.select_dtypes(include=[\"object\"]).nunique() > MAX_CARDINALITY\n",
    "    features = hi_c_cat[hi_c_cat].index.tolist()\n",
    "    return features\n",
    "\n",
    "\n",
    "def select_low_cardinality_categorical_features(df: pd.DataFrame):\n",
    "    lo_c_cat = df.select_dtypes(include=[\"object\"]).nunique() <= MAX_CARDINALITY\n",
    "    features = lo_c_cat[lo_c_cat].index.tolist()\n",
    "    return features\n",
    "\n",
    "\n",
    "def select_numerical_features(df: pd.DataFrame):\n",
    "    return df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "\n",
    "# Combine handling missing values and preprocessing into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"cat_low_cardinality\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"impute\",\n",
    "                        # SimpleImputer(strategy=\"most_frequent\"),\n",
    "                        SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"to_num\",\n",
    "                        OneHotEncoder(),\n",
    "                        # OrdinalEncoder(),\n",
    "                        # LabelEncoder(),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            select_low_cardinality_categorical_features,  # make_column_selector(dtype_include='object'),\n",
    "        ),\n",
    "        (\n",
    "            \"cat_high_cardinality\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"impute\",\n",
    "                        SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "                        # SimpleImputer(strategy=\"most_frequent\"),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"to_num\",\n",
    "                        # OneHotEncoder(),\n",
    "                        OrdinalEncoder(),\n",
    "                        # LabelEncoder(),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            select_high_cardinality_categorical_features,  # make_column_selector(dtype_include='object'),\n",
    "        ),\n",
    "        (\n",
    "            \"num\",\n",
    "            Pipeline(\n",
    "                steps=[\n",
    "                    (\n",
    "                        \"impute\",\n",
    "                        # KNNImputer(n_neighbors=1),\n",
    "                        # KNNImputer(n_neighbors=3),\n",
    "                        KNNImputer(n_neighbors=5),\n",
    "                        # SimpleImputer(strategy=\"mean\"),\n",
    "                        # SimpleImputer(strategy=\"median\"),\n",
    "                    ),\n",
    "                    (\n",
    "                        \"scale\",\n",
    "                        StandardScaler(),\n",
    "                        # MinMaxScaler(),\n",
    "                        # RobustScaler(),\n",
    "                    ),\n",
    "                ]\n",
    "            ),\n",
    "            select_numerical_features,  # make_column_selector(dtype_include='number'),\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    "    # sparse_threshold=0,\n",
    ")\n",
    "\n",
    "# preprocessor.set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data: pd.DataFrame, pipeline: Pipeline) -> pd.DataFrame:\n",
    "    X = data.drop(columns=[TARGET_COLUMN])\n",
    "    y = data[TARGET_COLUMN]\n",
    "\n",
    "    # Fit and transform the data using the pipeline\n",
    "    data_transformed = pipeline.fit_transform(X=X, y=y)\n",
    "\n",
    "    # Extract feature names from the preprocessor step\n",
    "    if \"preprocessor\" in pipeline.named_steps:\n",
    "        feature_names = pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "    else:\n",
    "        feature_names = data.columns\n",
    "\n",
    "    # Extract the selected feature indices from the feature engineering step\n",
    "    if \"feature_engineering\" in pipeline.named_steps:\n",
    "        feature_selector = pipeline.named_steps[\"feature_engineering\"].named_steps[\n",
    "            \"feature_selection\"\n",
    "        ]\n",
    "        if isinstance(feature_selector, SelectFromModel):\n",
    "            support_mask = feature_selector.get_support()\n",
    "        elif isinstance(feature_selector, RFE):\n",
    "            support_mask = feature_selector.support_\n",
    "        else:\n",
    "            support_mask = np.ones(len(feature_names), dtype=bool)\n",
    "        selected_feature_names = [\n",
    "            name for name, selected in zip(feature_names, support_mask) if selected\n",
    "        ]\n",
    "    else:\n",
    "        selected_feature_names = feature_names\n",
    "\n",
    "    # Remove prefixes from the column names\n",
    "    selected_feature_names = [name.split(\"__\")[-1] for name in selected_feature_names]\n",
    "\n",
    "    # Convert the transformed data back to a DataFrame\n",
    "    data_transformed_df = pd.DataFrame(data_transformed, columns=selected_feature_names)\n",
    "\n",
    "    data_transformed_df[TARGET_COLUMN] = y.values\n",
    "\n",
    "    return data_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to transform the train_data\n",
    "train_data_transformed_df = transform_data(train_data, pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Preprocessing Works\n",
    "\n",
    "- Check if no missing values after preprocessing\n",
    "- Check if all columns are numerical after preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "print(\"Number of missing values in transformed data:\")\n",
    "display(pd.DataFrame(train_data_transformed_df.isna().sum()).T)\n",
    "\n",
    "assert train_data_transformed_df.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all columns are numerical\n",
    "\n",
    "all_columns_numerical = train_data_transformed_df.select_dtypes(\n",
    "    include=[np.number]\n",
    ").columns.tolist()\n",
    "all_columns = train_data_transformed_df.columns.tolist()\n",
    "\n",
    "columns_not_numerical = (\n",
    "    set(all_columns) - set(all_columns_numerical) - set([TARGET_COLUMN])\n",
    ")\n",
    "print(f\"Columns not numerical: {columns_not_numerical}\")\n",
    "\n",
    "# Output the types of the non-numerical columns\n",
    "for col in columns_not_numerical:\n",
    "    print(\n",
    "        f\"Column: {col}, Type: {train_data_transformed_df[col].dtype}, First Value: {train_data_transformed_df[col].iloc[0]}\"\n",
    "    )\n",
    "\n",
    "assert columns_not_numerical == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = Pipeline(\n",
    "    steps=[\n",
    "        # (\"polynomial_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        # (\"feature_selection\", RFE(estimator=RandomForestClassifier(random_state=RANDOM_SEED))),\n",
    "        # (\"feature_selection\", RFE(estimator=RandomForestClassifier(random_state=RANDOM_SEED), n_features_to_select=10)),\n",
    "        (\n",
    "            \"feature_selection\",\n",
    "            SelectFromModel(LassoCV(cv=5, random_state=RANDOM_SEED, max_iter=10000)),\n",
    "        ),\n",
    "        # (\"feature_selection\", SelectKBest(f_classif, k=10)),\n",
    "        # (\"feature_selection\", SelectKBest(mutual_info_classif, k=10)),\n",
    "        # (\"feature_selection\", SelectKBest(f_classif, k=20)),\n",
    "        # (\"feature_selection\", SelectKBest(mutual_info_classif, k=20)),\n",
    "        # (\"feature_selection\", SelectFromModel(RandomForestClassifier(random_state=RANDOM_SEED), threshold=\"mean\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the feature engineering pipeline to the main pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_engineering\", feature_engineering),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to transform the train_data\n",
    "train_data_transformed_df = transform_data(train_data, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_transformed_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Correlation on Transformed Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_data_transformed_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".2f\",\n",
    "    xticklabels=train_data_transformed_df.columns.tolist(),\n",
    "    yticklabels=train_data_transformed_df.columns.tolist(),\n",
    ")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the correlation matrix to only include the Target Column\n",
    "target_corr_matrix = corr_matrix[[TARGET_COLUMN]].sort_values(\n",
    "    by=TARGET_COLUMN, ascending=False\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(target_corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title(f\"Correlation with {TARGET_COLUMN}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Grids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_engineering\", feature_engineering),\n",
    "        (\"classifier\", \"passthrough\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Note: \"passthrough\" is used as a placeholder for the model to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor Grids\n",
    "\n",
    "2 _ 2 _ 5 = 20\n",
    "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
    "11 min 51 s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor_grid = {\n",
    "#     \"preprocessor__cat_low_cardinality__impute\": [\n",
    "#         # SimpleImputer(strategy=\"most_frequent\"), #\n",
    "#         SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "#     ],\n",
    "#     \"preprocessor__cat_low_cardinality__to_num\": [\n",
    "#         OneHotEncoder(),\n",
    "#         OrdinalEncoder(), #\n",
    "#     ],\n",
    "#     \"preprocessor__cat_high_cardinality__impute\": [\n",
    "#         # SimpleImputer(strategy=\"most_frequent\"), #\n",
    "#         SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "#     ],\n",
    "#     \"preprocessor__cat_high_cardinality__to_num\": [\n",
    "#         OneHotEncoder(),  # TODO works better?\n",
    "#         OrdinalEncoder(), #\n",
    "#     ],\n",
    "#     \"preprocessor__num__impute\": [\n",
    "#         # KNNImputer(n_neighbors=1), #\n",
    "#         KNNImputer(n_neighbors=3), #\n",
    "#         KNNImputer(n_neighbors=5),  #\n",
    "#         SimpleImputer(strategy=\"mean\"), #\n",
    "#         SimpleImputer(strategy=\"median\"),\n",
    "#     ],\n",
    "#     \"preprocessor__num__scale\": [\n",
    "#         \"passthrough\",\n",
    "#         StandardScaler(),\n",
    "#         # MinMaxScaler(), #\n",
    "#         # RobustScaler(), #\n",
    "#     ],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_grid = {\n",
    "    \"preprocessor__cat__impute\": [\n",
    "        SimpleImputer(strategy=\"most_frequent\"),  #\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "    ],\n",
    "    \"preprocessor__cat__to_num\": [\n",
    "        OneHotEncoder(),\n",
    "        OrdinalEncoder(),  #\n",
    "    ],\n",
    "    \"preprocessor__num__impute\": [\n",
    "        # KNNImputer(n_neighbors=1), #\n",
    "        KNNImputer(n_neighbors=3),  #\n",
    "        KNNImputer(n_neighbors=5),  #\n",
    "        SimpleImputer(strategy=\"mean\"),  #\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "    ],\n",
    "    \"preprocessor__num__scale\": [\n",
    "        \"passthrough\",\n",
    "        StandardScaler(),\n",
    "        # MinMaxScaler(), #\n",
    "        # RobustScaler(), #\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASSO_CV = 5\n",
    "\n",
    "feature_engineering_grid = {\n",
    "    \"create_features\": [\n",
    "        FunctionTransformer(create_features),\n",
    "        # \"passthrough\",\n",
    "    ],\n",
    "    \"feature_engineering__feature_selection\": [\n",
    "        # RFE(estimator=RandomForestClassifier(random_state=RANDOM_SEED)),\n",
    "        # RFE(estimator=RandomForestClassifier(random_state=RANDOM_SEED), n_features_to_select=10),\n",
    "        SelectFromModel(LassoCV(cv=LASSO_CV, random_state=RANDOM_SEED)),\n",
    "        # SelectKBest(f_classif, k=10),\n",
    "        # SelectKBest(mutual_info_classif, k=10),\n",
    "        # SelectKBest(f_classif, k=20),\n",
    "        # SelectKBest(mutual_info_classif, k=20),\n",
    "        # SelectFromModel(RandomForestClassifier(random_state=RANDOM_SEED), threshold=\"mean\"),\n",
    "        \"passthrough\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Grid\n",
    "\n",
    "6 + 3 + 6 + 6 + 8 = 29\n",
    "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
    "3 min 45s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grids = [\n",
    "    # {\n",
    "    #     # Logistic Regression\n",
    "    #     \"classifier\": [LogisticRegression()],\n",
    "    #     \"classifier__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    #     \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "    #     \"classifier__solver\": [\"liblinear\", \"saga\"],\n",
    "    # },\n",
    "    # {\n",
    "    #     # Decision Tree\n",
    "    #     \"classifier\": [DecisionTreeClassifier(random_state=RANDOM_SEED)],\n",
    "    #     \"classifier__max_depth\": [None, 10, 20, 30],\n",
    "    #     \"classifier__min_samples_split\": [2, 5, 10],\n",
    "    #     \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "    # },\n",
    "    # {\n",
    "    #     # Random Forest\n",
    "    #     \"classifier\": [RandomForestClassifier(random_state=RANDOM_SEED)],\n",
    "    #     \"classifier__n_estimators\": [100, 200, 300],\n",
    "    #     \"classifier__max_depth\": [None, 10, 20, 30],\n",
    "    #     \"classifier__min_samples_split\": [2, 5, 10],\n",
    "    #     \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "    # },\n",
    "    # {\n",
    "    #     # K-Nearest Neighbors\n",
    "    #     \"classifier\": [KNeighborsClassifier()],\n",
    "    #     \"classifier__n_neighbors\": [3, 5, 7, 9, 11],\n",
    "    #     \"classifier__weights\": [\"uniform\", \"distance\"],\n",
    "    #     \"classifier__metric\": [\"euclidean\", \"manhattan\"],\n",
    "    # },\n",
    "    # {\n",
    "    #     # Support Vector Machine\n",
    "    #     \"classifier\": [SVC(probability=True)],\n",
    "    #     \"classifier__C\": [0.01, 0.1, 1, 10],\n",
    "    #     \"classifier__kernel\": [\"linear\", \"rbf\", \"poly\"],\n",
    "    #     \"classifier__gamma\": [\"scale\", \"auto\"],\n",
    "    # },\n",
    "    # {\n",
    "    #     # Gradient Boosting\n",
    "    #     \"classifier\": [GradientBoostingClassifier(random_state=RANDOM_SEED)],\n",
    "    #     \"classifier__n_estimators\": [100, 150, 200, 250, 300, 500],\n",
    "    #     \"classifier__learning_rate\": [0.05, 0.1, 0.15, 0.2],\n",
    "    #     \"classifier__max_depth\": [3, 5, 7],\n",
    "    #     \"classifier__subsample\": [0.8, 1.0],\n",
    "    # },\n",
    "    # {\n",
    "    #     # XGBoost\n",
    "    #     \"classifier\": [XGBClassifier(random_state=RANDOM_SEED)],\n",
    "    #     \"classifier__n_estimators\": [100, 250, 500],\n",
    "    #     \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    #     \"classifier__max_depth\": [3, 6, 9],\n",
    "    #     \"classifier__subsample\": [0.8, 1.0],\n",
    "    #     \"classifier__colsample_bytree\": [0.8, 1.0],\n",
    "    # },\n",
    "    # {\n",
    "    #     # LightGBM\n",
    "    #     \"classifier\": [LGBMClassifier(random_state=RANDOM_SEED, verbose=-1)],\n",
    "    #     \"classifier__n_estimators\": [100, 250, 500],\n",
    "    #     \"classifier__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    #     \"classifier__max_depth\": [3, 6, 9],\n",
    "    #     \"classifier__subsample\": [0.8, 1.0],\n",
    "    #     \"classifier__colsample_bytree\": [0.8, 1.0],\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grids = [\n",
    "    {\n",
    "        # Gradient Boosting\n",
    "        \"classifier\": [GradientBoostingClassifier(random_state=RANDOM_SEED)],\n",
    "        \"classifier__n_estimators\": [200, 250, 300],\n",
    "        \"classifier__learning_rate\": [0.05, 0.1, 0.15],\n",
    "        \"classifier__max_depth\": [1, 3, 5],\n",
    "        \"classifier__subsample\": [0.8, 1.0],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grids = []\n",
    "\n",
    "for m in model_grids:\n",
    "    grid = m\n",
    "    grid.update(preprocessor_grid)\n",
    "    grid.update(feature_engineering_grid)\n",
    "    parameter_grids.append(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Parameter Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the train data into training and validation sets\n",
    "# X = train_data.drop(columns=[TARGET_COLUMN])\n",
    "# y = train_data[TARGET_COLUMN]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#     X, y, test_size=VALIDATION_SIZE, random_state=RANDOM_SEED\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train data into training and validation sets\n",
    "X_train = train_data.drop(columns=[TARGET_COLUMN])\n",
    "y_train = train_data[TARGET_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run experiments\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_grid=parameter_grids,\n",
    "#     cv=5,  # TODO parametrize\n",
    "#     scoring=\"accuracy\",\n",
    "#     verbose=1,\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"create_features\", FunctionTransformer(create_features)),\n",
    "        (\"clean_data\", FunctionTransformer(clean_data)),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_engineering\", feature_engineering),\n",
    "        (\"classifier\", GradientBoostingClassifier(random_state=RANDOM_SEED)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=10000, random_state=RANDOM_SEED),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"SVC\": SVC(max_iter=10000, random_state=RANDOM_SEED, probability=True),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    # \"XGBoost\": XGBClassifier(random_state=RANDOM_SEED),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=RANDOM_SEED, verbose=-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = {\n",
    "    \"constant\": SimpleImputer(strategy=\"constant\", fill_value=MISSING_VALUE),\n",
    "    \"most_frequent\": SimpleImputer(strategy=\"most_frequent\"),\n",
    "    \"onehot\": OneHotEncoder(),\n",
    "    \"ordinal\": OrdinalEncoder(),\n",
    "    \"knn_3\": KNNImputer(n_neighbors=3),\n",
    "    \"knn_5\": KNNImputer(n_neighbors=5),\n",
    "    \"mean\": SimpleImputer(strategy=\"mean\"),\n",
    "    \"median\": SimpleImputer(strategy=\"median\"),\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"create_features\": FunctionTransformer(create_features),\n",
    "    \"lasso\": SelectFromModel(LassoCV(cv=5, random_state=RANDOM_SEED, max_iter=10000)),\n",
    "    \"passthrough\": \"passthrough\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_FOLDS = 5\n",
    "\n",
    "N_TRIALS_PIPELINE = 50000\n",
    "# N_TRIALS_PIPELINE = 100\n",
    "\n",
    "N_TRIALS_HYPERPARAMETERS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level to INFO\n",
    "optuna.logging.set_verbosity(optuna.logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     # Define the hyperparameters to tune\n",
    "#     params = {\n",
    "#         \"classifier__n_estimators\": trial.suggest_int(\"classifier__n_estimators\", 200, 300),\n",
    "#         \"classifier__learning_rate\": trial.suggest_float(\"classifier__learning_rate\", 0.05, 0.15),\n",
    "#         \"classifier__max_depth\": trial.suggest_int(\"classifier__max_depth\", 1, 5),\n",
    "#         \"classifier__subsample\": trial.suggest_float(\"classifier__subsample\", 0.8, 1.0),\n",
    "#         \"preprocessor__cat_low_cardinality__impute\": transformers[trial.suggest_categorical(\n",
    "#             \"preprocessor__cat_low_cardinality__impute\", [\"constant\", \"most_frequent\"]\n",
    "#         )],\n",
    "#         \"preprocessor__cat_low_cardinality__to_num\": transformers[trial.suggest_categorical(\n",
    "#             \"preprocessor__cat_low_cardinality__to_num\", [\"onehot\", \"ordinal\"]\n",
    "#         )],\n",
    "#         \"preprocessor__cat_high_cardinality__impute\": transformers[trial.suggest_categorical(\n",
    "#             \"preprocessor__cat_high_cardinality__impute\", [\"constant\", \"most_frequent\"]\n",
    "#         )],\n",
    "#         \"preprocessor__cat_high_cardinality__to_num\": transformers[trial.suggest_categorical(\n",
    "#             \"preprocessor__cat_high_cardinality__to_num\", [\"onehot\", \"ordinal\"]\n",
    "#         )],\n",
    "#         \"preprocessor__num__impute\": transformers[trial.suggest_categorical(\n",
    "#             \"preprocessor__num__impute\", [\"knn_3\", \"knn_5\", \"mean\", \"median\"]\n",
    "#         )],\n",
    "#         \"preprocessor__num__scale\": transformers[trial.suggest_categorical(\n",
    "#             \"preprocessor__num__scale\", [\"standard\", \"minmax\", \"robust\", \"passthrough\"]\n",
    "#         )],\n",
    "#         \"feature_engineering__feature_selection\": transformers[trial.suggest_categorical(\n",
    "#             \"feature_engineering__feature_selection\", [\"lasso\", \"passthrough\"]\n",
    "#         )],\n",
    "#     }\n",
    "\n",
    "#     # Update the pipeline with the suggested hyperparameters\n",
    "#     pipeline.set_params(**params)\n",
    "\n",
    "#     # Perform cross-validation\n",
    "#     scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "#     return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import signal\n",
    "\n",
    "# class TimeoutException(Exception):\n",
    "#     pass\n",
    "\n",
    "# def timeout_handler(signum, frame):\n",
    "#     raise TimeoutException\n",
    "\n",
    "# def objective(trial):\n",
    "#     # Set the timeout handler\n",
    "#     signal.signal(signal.SIGALRM, timeout_handler)\n",
    "#     signal.alarm(120)  # Set the timeout to 2 minutes (120 seconds)\n",
    "\n",
    "#     try:\n",
    "#         # Define the models to tune\n",
    "#         model_name = trial.suggest_categorical(\"classifier\", [\"LogisticRegression\", \"RandomForest\", \"GradientBoosting\", \"SVC\", \"KNeighbors\"])\n",
    "\n",
    "#         if model_name == \"LogisticRegression\":\n",
    "#             classifier = LogisticRegression(\n",
    "#                 C=trial.suggest_float(\"classifier__C\", 0.01, 10.0),\n",
    "#                 penalty=trial.suggest_categorical(\"classifier__penalty\", [\"l1\", \"l2\"]),\n",
    "#                 solver=trial.suggest_categorical(\"classifier__solver\", [\"liblinear\", \"saga\"]),\n",
    "#                 random_state=RANDOM_SEED\n",
    "#             )\n",
    "#         elif model_name == \"RandomForest\":\n",
    "#             classifier = RandomForestClassifier(\n",
    "#                 n_estimators=trial.suggest_int(\"classifier__n_estimators\", 100, 300),\n",
    "#                 max_depth=trial.suggest_int(\"classifier__max_depth\", 1, 10),\n",
    "#                 min_samples_split=trial.suggest_int(\"classifier__min_samples_split\", 2, 10),\n",
    "#                 min_samples_leaf=trial.suggest_int(\"classifier__min_samples_leaf\", 1, 4),\n",
    "#                 random_state=RANDOM_SEED\n",
    "#             )\n",
    "#         elif model_name == \"KNeighbors\":\n",
    "#             classifier = KNeighborsClassifier(\n",
    "#                 n_neighbors=trial.suggest_int(\"classifier__n_neighbors\", 3, 11),\n",
    "#                 weights=trial.suggest_categorical(\"classifier__weights\", [\"uniform\", \"distance\"]),\n",
    "#                 metric=trial.suggest_categorical(\"classifier__metric\", [\"euclidean\", \"manhattan\"])\n",
    "#             )\n",
    "#         elif model_name == \"SVC\":\n",
    "#             classifier = SVC(\n",
    "#                 C=trial.suggest_float(\"classifier__C\", 0.01, 10.0),\n",
    "#                 kernel=trial.suggest_categorical(\"classifier__kernel\", [\"linear\", \"rbf\", \"poly\"]),\n",
    "#                 probability=True,\n",
    "#                 max_iter=1000,\n",
    "#                 random_state=RANDOM_SEED\n",
    "#             )\n",
    "#         elif model_name == \"GradientBoosting\":\n",
    "#             classifier = GradientBoostingClassifier(\n",
    "#                 n_estimators=trial.suggest_int(\"classifier__n_estimators\", 100, 300),\n",
    "#                 learning_rate=trial.suggest_float(\"classifier__learning_rate\", 0.01, 0.2),\n",
    "#                 max_depth=trial.suggest_int(\"classifier__max_depth\", 1, 10),\n",
    "#                 subsample=trial.suggest_float(\"classifier__subsample\", 0.8, 1.0),\n",
    "#                 random_state=RANDOM_SEED\n",
    "#             )\n",
    "\n",
    "#         # Define the hyperparameters for the preprocessor and feature engineering\n",
    "#         params = {\n",
    "#             \"preprocessor__cat__impute\": transformers[trial.suggest_categorical(\n",
    "#                 \"preprocessor__cat__impute\", [\"constant\", \"most_frequent\"]\n",
    "#             )],\n",
    "#             \"preprocessor__cat__to_num\": transformers[trial.suggest_categorical(\n",
    "#                 \"preprocessor__cat__to_num\", [\"onehot\", \"ordinal\"]\n",
    "#             )],\n",
    "#             \"preprocessor__num__impute\": transformers[trial.suggest_categorical(\n",
    "#                 \"preprocessor__num__impute\", [\"knn_3\", \"knn_5\", \"mean\", \"median\"]\n",
    "#             )],\n",
    "#             \"preprocessor__num__scale\": transformers[trial.suggest_categorical(\n",
    "#                 \"preprocessor__num__scale\", [\"standard\", \"passthrough\"]\n",
    "#                 # \"preprocessor__num__scale\", [\"standard\", \"minmax\", \"robust\", \"passthrough\"]\n",
    "#             )],\n",
    "#             \"create_features\": transformers[trial.suggest_categorical(\n",
    "#                 \"create_features\", [\"create_features\", \"passthrough\"]\n",
    "#             )],\n",
    "#             \"feature_engineering__feature_selection\": transformers[trial.suggest_categorical(\n",
    "#                 \"feature_engineering__feature_selection\", [\"lasso\", \"passthrough\"]\n",
    "#             )],\n",
    "#         }\n",
    "\n",
    "#         # Update the pipeline with the suggested hyperparameters\n",
    "#         pipeline.set_params(classifier=classifier, **params)\n",
    "\n",
    "#         # Print the parameters for the current trial\n",
    "#         print(f\"Trial {trial.number}: Starting\")\n",
    "#         for key, value in trial.params.items():\n",
    "#             print(f\"  {key}: {value}\")\n",
    "\n",
    "#         # Perform cross-validation\n",
    "#         scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "#         # Cancel the alarm\n",
    "#         signal.alarm(0)\n",
    "\n",
    "#         return scores.mean()\n",
    "\n",
    "#     except TimeoutException:\n",
    "#         print(f\"Trial {trial.number}: Timeout\")\n",
    "#         return 0.0  # Return a bad accuracy score if the trial times out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Set the timeout handler\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(120)  # Set the timeout to 2 minutes (120 seconds)\n",
    "\n",
    "    try:\n",
    "        # Define the hyperparameters for the preprocessor and feature engineering\n",
    "        params = {\n",
    "            \"classifier\": classifiers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"classifier\",\n",
    "                    [\n",
    "                        \"LogisticRegression\",\n",
    "                        \"RandomForest\",\n",
    "                        \"GradientBoosting\",\n",
    "                        \"SVC\",\n",
    "                        \"KNeighbors\",\n",
    "                        \"XGBoost\",\n",
    "                        \"LightGBM\",\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_low_cardinality__impute\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_low_cardinality__impute\",\n",
    "                    [\"constant\", \"most_frequent\"],\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_low_cardinality__to_num\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_low_cardinality__to_num\", [\"onehot\", \"ordinal\"]\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_high_cardinality__impute\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_high_cardinality__impute\",\n",
    "                    [\"constant\", \"most_frequent\"],\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_high_cardinality__to_num\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_high_cardinality__to_num\", [\"onehot\", \"ordinal\"]\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__num__impute\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__num__impute\", [\"knn_3\", \"knn_5\", \"mean\", \"median\"]\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__num__scale\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__num__scale\",\n",
    "                    [\"standard\", \"passthrough\"],\n",
    "                )\n",
    "            ],\n",
    "            # TODO get selected features selection here subset of selected features list\n",
    "            \"create_features\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"create_features\", [\"create_features\", \"passthrough\"]\n",
    "                )\n",
    "            ],\n",
    "            \"feature_engineering__feature_selection\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"feature_engineering__feature_selection\", [\"lasso\", \"passthrough\"]\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        if params[\"create_features\"] != \"passthrough\":\n",
    "            params[\"create_features\"] = FunctionTransformer(\n",
    "                create_features,\n",
    "                kw_args={\n",
    "                    f\"use_{feature}\": trial.suggest_categorical(\n",
    "                        f\"create_features__kw_args__use_{feature}\", [False, True]\n",
    "                    )\n",
    "                    for feature in CREATED_FEATURES\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # Update the pipeline with the suggested hyperparameters\n",
    "        pipeline.set_params(**params)\n",
    "\n",
    "        # Print the parameters for the current trial\n",
    "        print(f\"Trial {trial.number}: Starting\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train, y_train, cv=CV_FOLDS, scoring=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        # Cancel the alarm\n",
    "        signal.alarm(0)\n",
    "\n",
    "        trial.set_user_attr(\"pipeline\", pipeline)\n",
    "\n",
    "        return scores.mean()\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\"Trial {trial.number}: Timeout\")\n",
    "        return 0.0  # Return a bad accuracy score if the trial times out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"classifier\": [\n",
    "        # \"LogisticRegression\",\n",
    "        # \"RandomForest\",\n",
    "        \"GradientBoosting\",\n",
    "        # \"SVC\",\n",
    "        # \"KNeighbors\",\n",
    "        # \"XGBoost\",\n",
    "        # \"LightGBM\",\n",
    "    ],\n",
    "    \"preprocessor__cat_low_cardinality__impute\": [\"constant\"],  # , \"most_frequent\"],\n",
    "    \"preprocessor__cat_low_cardinality__to_num\": [\"ordinal\"],  # \"onehot\" ],\n",
    "    \"preprocessor__cat_high_cardinality__impute\": [\"most_frequent\"],  # ,  \"constant\"],\n",
    "    \"preprocessor__cat_high_cardinality__to_num\": [\"onehot\"],  # , \"onehot\"],\n",
    "    \"preprocessor__num__impute\": [\"knn_5\"],  # \"knn_3\", \"knn_5\", \"mean\", \"median\"],\n",
    "    \"preprocessor__num__scale\": [\"standard\"],  # , \"passthrough\"],\n",
    "    \"feature_engineering__feature_selection\": [\"lasso\"],  # , \"passthrough\"],\n",
    "    \"create_features\": [\"create_features\"],  # , \"passthrough\",],\n",
    "    # **{\n",
    "    #     f\"create_features__kw_args__use_{feature}\": [False, True]\n",
    "    #     for feature in CREATED_FEATURES\n",
    "    # },\n",
    "    \"create_features__kw_args__use_AmountSpentTotal\": [False],\n",
    "    \"create_features__kw_args__use_CabinDeck\": [True],\n",
    "    \"create_features__kw_args__use_CabinNumber\": [True],\n",
    "    \"create_features__kw_args__use_CabinSide\": [True],\n",
    "    \"create_features__kw_args__use_CabinMates\": [True],\n",
    "    \"create_features__kw_args__use_PassengerGroupSize\": [True],\n",
    "}\n",
    "\n",
    "# param_grid = {\n",
    "#     \"classifier\": [\"LightGBM\"],\n",
    "#     \"preprocessor__cat_low_cardinality__impute\": [\"most_frequent\"],\n",
    "#     \"preprocessor__cat_low_cardinality__to_num\": [\"onehot\"],\n",
    "#     \"preprocessor__cat_high_cardinality__impute\": [\"most_frequent\"],\n",
    "#     \"preprocessor__cat_high_cardinality__to_num\": [\"ordinal\"],\n",
    "#     \"preprocessor__num__impute\": [\"knn_5\"],\n",
    "#     \"preprocessor__num__scale\": [\"standard\"],\n",
    "#     \"create_features\": [\"create_features\"],\n",
    "#     \"feature_engineering__feature_selection\": [\"lasso\"],\n",
    "#     \"create_features__kw_args__use_AmountSpentTotal\": [True],\n",
    "#     \"create_features__kw_args__use_CabinDeck\": [True],\n",
    "#     \"create_features__kw_args__use_CabinNumber\": [False],\n",
    "#     \"create_features__kw_args__use_CabinSide\": [False],\n",
    "#     \"create_features__kw_args__use_CabinMates\": [True],\n",
    "#     \"create_features__kw_args__use_PassengerGroupSize\": [False],\n",
    "#     \"classifier__n_estimators\": [\"158\"],\n",
    "#     \"classifier__learning_rate\": [\"0.11333262719293624\"],\n",
    "#     \"classifier__max_depth\": [\"2\"],\n",
    "#     \"classifier__subsample\": [\"0.7620188236959264\"],\n",
    "#     \"classifier__colsample_bytree\": [\"0.8358468899985141\"],\n",
    "# }\n",
    "\n",
    "# TODO\n",
    "# # Create a study and optimize the objective function\n",
    "# study_pipeline = optuna.create_study(\n",
    "#     direction=\"maximize\", sampler=GridSampler(param_grid)\n",
    "# )\n",
    "# study_pipeline.optimize(objective, n_trials=N_TRIALS_PIPELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters(params):\n",
    "    for k, v in params.items():\n",
    "        print(f\"  {k:<50}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show best pipeline\n",
    "# print(\"Best pipeline:\")\n",
    "# print_model_parameters(study_pipeline.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial, pipeline, classifier_name):\n",
    "    classifier = classifiers[classifier_name]\n",
    "\n",
    "    # if classifier_name == \"LogisticRegression\":\n",
    "    #     classifier.set_params(\n",
    "    #         C=trial.suggest_float(\"classifier__C\", 0.01, 10.0),\n",
    "    #         penalty=trial.suggest_categorical(\"classifier__penalty\", [\"l1\", \"l2\"]),\n",
    "    #         solver=trial.suggest_categorical(\n",
    "    #             \"classifier__solver\", [\"liblinear\", \"saga\"]\n",
    "    #         ),\n",
    "    #     )\n",
    "    # elif classifier_name == \"RandomForest\":\n",
    "    #     classifier.set_params(\n",
    "    #         n_estimators=trial.suggest_int(\"classifier__n_estimators\", 100, 300),\n",
    "    #         max_depth=trial.suggest_categorical(\n",
    "    #             \"classifier__max_depth\", [-1, 5, 10, 20]\n",
    "    #         ),\n",
    "    #         min_samples_split=trial.suggest_int(\"classifier__min_samples_split\", 2, 10),\n",
    "    #         min_samples_leaf=trial.suggest_int(\"classifier__min_samples_leaf\", 1, 4),\n",
    "    #     )\n",
    "    # elif classifier_name == \"KNeighbors\":\n",
    "    #     classifier.set_params(\n",
    "    #         n_neighbors=trial.suggest_int(\"classifier__n_neighbors\", 3, 11),\n",
    "    #         weights=trial.suggest_categorical(\n",
    "    #             \"classifier__weights\", [\"uniform\", \"distance\"]\n",
    "    #         ),\n",
    "    #         metric=trial.suggest_categorical(\n",
    "    #             \"classifier__metric\", [\"euclidean\", \"manhattan\"]\n",
    "    #         ),\n",
    "    #     )\n",
    "    # elif classifier_name == \"SVC\":\n",
    "    #     classifier.set_params(\n",
    "    #         C=trial.suggest_float(\"classifier__C\", 0.01, 10.0),\n",
    "    #         kernel=trial.suggest_categorical(\n",
    "    #             \"classifier__kernel\", [\"linear\", \"rbf\", \"poly\"]\n",
    "    #         ),\n",
    "    #         probability=True,\n",
    "    #         max_iter=1000,\n",
    "    #     )\n",
    "    GradientBoostingClassifier\n",
    "    if classifier_name == \"GradientBoosting\":\n",
    "        MAX_DEPTH = 10\n",
    "        max_depth = trial.suggest_int(\"classifier__max_depth\", 1, MAX_DEPTH)\n",
    "        classifier.set_params(\n",
    "            criterion=trial.suggest_categorical(\"classifier__criterion\", [\"friedman_mse\", \"squared_error\"]),\n",
    "            learning_rate=trial.suggest_float(\"classifier__learning_rate\", 0.01, 0.3, log=True),\n",
    "            max_depth=max_depth if max_depth != MAX_DEPTH else None,\n",
    "            # max_features=trial.suggest_categorical(\"classifier__max_features\", [None, \"sqrt\", \"log2\"]),\n",
    "            # min_samples_leaf=trial.suggest_int(\"classifier__min_samples_leaf\", 1, 50),\n",
    "            # min_samples_split=trial.suggest_int(\"classifier__min_samples_split\", 2, 50),\n",
    "            n_estimators=trial.suggest_int(\"classifier__n_estimators\", 100, 1000, log=True),\n",
    "            subsample=trial.suggest_float(\"classifier__subsample\", 0.7, 1.0, step=0.1),\n",
    "        )\n",
    "    # elif classifier_name == \"XGBoost\":\n",
    "    #     classifier.set_params(\n",
    "    #         n_estimators=trial.suggest_int(\"classifier__n_estimators\", 50, 300),\n",
    "    #         learning_rate=trial.suggest_float(\"classifier__learning_rate\", 0.01, 0.2),\n",
    "    #         max_depth=trial.suggest_categorical(\n",
    "    #             \"classifier__max_depth\", [-1, 5, 10, 20]\n",
    "    #         ),\n",
    "    #         subsample=trial.suggest_float(\"classifier__subsample\", 0.8, 1.0),\n",
    "    #         colsample_bytree=trial.suggest_float(\n",
    "    #             \"classifier__colsample_bytree\", 0.8, 1.0\n",
    "    #         ),\n",
    "    #     )\n",
    "    # elif classifier_name == \"LightGBM\":\n",
    "    #     classifier.set_params(\n",
    "    #         n_estimators=trial.suggest_int(\"classifier__n_estimators\", 50, 300),\n",
    "    #         learning_rate=trial.suggest_float(\"classifier__learning_rate\", 0.01, 0.2),\n",
    "    #         max_depth=trial.suggest_categorical(\n",
    "    #             \"classifier__max_depth\", [-1, 5, 10, 20]\n",
    "    #         ),\n",
    "    #         subsample=trial.suggest_float(\"classifier__subsample\", 0.7, 1.0),\n",
    "    #         colsample_bytree=trial.suggest_float(\n",
    "    #             \"classifier__colsample_bytree\", 0.8, 1.0\n",
    "    #         ),\n",
    "    #     )\n",
    "\n",
    "    # Update the pipeline with the classifier\n",
    "    pipeline.set_params(classifier=classifier)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(\n",
    "        pipeline, X_train, y_train, cv=CV_FOLDS, scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"pipeline\", pipeline)\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_and_set_params(pipeline, study_params):\n",
    "#     mapped_params = {\n",
    "#         key: (\n",
    "#             transformers[value]\n",
    "#             if value in transformers\n",
    "#             else classifiers[value] if key == \"classifier\"\n",
    "#             else value\n",
    "#         )\n",
    "#         for key, value in study_params.items()\n",
    "#     }\n",
    "#     print(mapped_params)\n",
    "#     pipeline.set_params(**mapped_params)\n",
    "#     return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# # Get the best classifier name from the previous study\n",
    "# best_classifier_name = study_pipeline.best_params[\"classifier\"]\n",
    "# # best_pipeline = map_and_set_params(pipeline, study_pipeline.best_params)\n",
    "# best_pipeline = study_pipeline.best_trial.user_attrs[\"pipeline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# # Create a study and optimize the objective function\n",
    "# study_hyperparameters = optuna.create_study(\n",
    "#     direction=\"maximize\", sampler=TPESampler(seed=RANDOM_SEED)\n",
    "# )\n",
    "# study_hyperparameters.optimize(\n",
    "#     lambda trial: objective(trial, pipeline, best_classifier_name),\n",
    "#     n_trials=N_TRIALS_HYPERPARAMETERS,\n",
    "#     # n_trials=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the best hyperparameters\n",
    "# print(\"Best hyperparameters from second study:\")\n",
    "# print_model_parameters(study_hyperparameters.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best so far\n",
    "\n",
    "```\n",
    "Best Model:\n",
    "classifier                                        : GradientBoostingClassifier(random_state=42)\n",
    "classifier__learning_rate                         : 0.1\n",
    "classifier__n_estimators                          : 250\n",
    "preprocessor__cat_onehot__impute                  : SimpleImputer(fill_value='Missing', strategy='constant')\n",
    "preprocessor__cat_onehot__onehot                  : OneHotEncoder()\n",
    "preprocessor__cat_ordinal__impute                 : SimpleImputer(fill_value='Missing', strategy='constant')\n",
    "preprocessor__cat_ordinal__ordinal                : OneHotEncoder()\n",
    "preprocessor__num__impute                         : SimpleImputer(strategy='median')\n",
    "preprocessor__num__scale                          : StandardScaler()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Set the timeout handler\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(180)  # Set the timeout to 3 minutes (180 seconds)\n",
    "\n",
    "    try:\n",
    "        # Define the hyperparameters for the preprocessor and feature engineering\n",
    "        params = {\n",
    "            \"preprocessor__cat_low_cardinality__impute\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_low_cardinality__impute\",\n",
    "                    [\"constant\", \"most_frequent\"],\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_low_cardinality__to_num\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_low_cardinality__to_num\", [\"onehot\", \"ordinal\"]\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_high_cardinality__impute\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_high_cardinality__impute\",\n",
    "                    [\"constant\", \"most_frequent\"],\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__cat_high_cardinality__to_num\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__cat_high_cardinality__to_num\", [\"onehot\", \"ordinal\"]\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__num__impute\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__num__impute\", [\"knn_3\", \"knn_5\", \"mean\", \"median\"]\n",
    "                )\n",
    "            ],\n",
    "            \"preprocessor__num__scale\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"preprocessor__num__scale\",\n",
    "                    [\"standard\", \"passthrough\"],\n",
    "                )\n",
    "            ],\n",
    "            \"create_features\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"create_features\", [\"create_features\", \"passthrough\"]\n",
    "                )\n",
    "            ],\n",
    "            \"feature_engineering__feature_selection\": transformers[\n",
    "                trial.suggest_categorical(\n",
    "                    \"feature_engineering__feature_selection\", [\"lasso\", \"passthrough\"]\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        if params[\"create_features\"] != \"passthrough\":\n",
    "            params[\"create_features\"] = FunctionTransformer(\n",
    "                create_features,\n",
    "                kw_args={\n",
    "                    f\"use_{feature}\": trial.suggest_categorical(\n",
    "                        f\"create_features__kw_args__use_{feature}\", [False, True]\n",
    "                    )\n",
    "                    for feature in CREATED_FEATURES\n",
    "                },\n",
    "            )\n",
    "        \n",
    "        classifier = GradientBoostingClassifier(random_state=RANDOM_SEED)\n",
    "        \n",
    "        MAX_DEPTH = 10\n",
    "        max_depth = trial.suggest_int(\"classifier__max_depth\", 1, MAX_DEPTH)\n",
    "        classifier.set_params(\n",
    "            criterion=trial.suggest_categorical(\"classifier__criterion\", [\"friedman_mse\", \"squared_error\"]),\n",
    "            learning_rate=trial.suggest_float(\"classifier__learning_rate\", 0.01, 0.3, log=True),\n",
    "            max_depth=max_depth if max_depth != MAX_DEPTH else None,\n",
    "            # max_features=trial.suggest_categorical(\"classifier__max_features\", [None, \"sqrt\", \"log2\"]),\n",
    "            # min_samples_leaf=trial.suggest_int(\"classifier__min_samples_leaf\", 1, 50),\n",
    "            # min_samples_split=trial.suggest_int(\"classifier__min_samples_split\", 2, 50),\n",
    "            n_estimators=trial.suggest_int(\"classifier__n_estimators\", 100, 1000, log=True),\n",
    "            subsample=trial.suggest_float(\"classifier__subsample\", 0.7, 1.0, step=0.1),\n",
    "        )\n",
    "\n",
    "        # Update the pipeline with the suggested hyperparameters\n",
    "        pipeline.set_params(**params)\n",
    "\n",
    "        # Update the pipeline with the classifier\n",
    "        pipeline.set_params(classifier=classifier)\n",
    "\n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train, y_train, cv=CV_FOLDS, scoring=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        trial.set_user_attr(\"pipeline\", pipeline)\n",
    "\n",
    "        # Print the parameters for the current trial\n",
    "        print(f\"Trial {trial.number}: Starting\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        # Perform cross-validation\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train, y_train, cv=CV_FOLDS, scoring=\"accuracy\"\n",
    "        )\n",
    "\n",
    "        # Cancel the alarm\n",
    "        signal.alarm(0)\n",
    "\n",
    "        trial.set_user_attr(\"pipeline\", pipeline)\n",
    "\n",
    "        return scores.mean()\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\"Trial {trial.number}: Timeout\")\n",
    "        return 0.0  # Return a bad accuracy score if the trial times out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\", sampler=TPESampler(seed=RANDOM_SEED)\n",
    ")\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial),\n",
    "    n_trials=N_TRIALS_HYPERPARAMETERS,\n",
    "    # n_trials=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model for current execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_\n",
    "best_params = study.best_params\n",
    "# best_params = study_pipeline.best_params | study_hyperparameters.best_params\n",
    "print(\"Best Model:\")\n",
    "print_model_parameters(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All models for current execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming grid_search is your GridSearchCV object\n",
    "# all_estimators_with_scores = list(\n",
    "#     zip(grid_search.cv_results_[\"params\"], grid_search.cv_results_[\"mean_test_score\"])\n",
    "# )\n",
    "\n",
    "# # Sort the estimators by their scores in descending order\n",
    "# all_estimators_with_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# # Print all estimators with their scores and ranking\n",
    "# for rank, (estimator, score) in enumerate(all_estimators_with_scores, start=1):\n",
    "#     print(f\"Rank: {rank}\")\n",
    "#     print(f\"Accuracy: {score}\")\n",
    "#     print(\"Model:\")\n",
    "#     print_model_parameters(estimator)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best (in train) model parameters to a JSON file\n",
    "best_params_file = f\"{DATA_DIR}/best_params_train.json\"\n",
    "\n",
    "# Convert non-serializable objects to their string representations\n",
    "serializable_best_params = {k: str(v) for k, v in best_params.items()}\n",
    "\n",
    "with open(best_params_file, \"w\") as f:\n",
    "    json.dump(serializable_best_params, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Evaluation with Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(pipeline, estimator, X_val, y_val):\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    y_pred_proba = (\n",
    "        pipeline.predict_proba(X_val)[:, 1]\n",
    "        if hasattr(pipeline, \"predict_proba\")\n",
    "        else None\n",
    "    )\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else None\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"ROC AUC: {roc_auc}\")\n",
    "    # print(classification_report(y_val, y_pred))\n",
    "    print(\"Model:\")\n",
    "    print_model_parameters(estimator)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate all estimators in grid search with validation set\n",
    "# for estimator, _ in all_estimators_with_scores:\n",
    "#     pipeline.set_params(**estimator)\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "#     score = evaluate_model(pipeline, estimator, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Training and Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_pipeline = map_and_set_params(pipeline, best_params)\n",
    "best_pipeline = study.best_trial.user_attrs[\"pipeline\"]\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "X_test = test_data\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "test_data[TARGET_COLUMN] = y_pred.astype(bool)\n",
    "\n",
    "# Make predictions on the test set with the best model\n",
    "# best_model = max(best_models.items(), key=lambda x: cross_val_score(x[1], X_train, y_train, cv=5).mean())[1]\n",
    "# test_predictions = best_model.predict(test_data)\n",
    "# test_data[TARGET_COLUMN] = test_predictions.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with only the ID_COLUMN and Predictions\n",
    "predictions_df = test_data.reset_index()[[ID_COLUMN, TARGET_COLUMN]]\n",
    "\n",
    "# Print predictions\n",
    "print(predictions_df)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv(f\"{DATA_DIR}/predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceship-titanic-UUL0AmCy-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
